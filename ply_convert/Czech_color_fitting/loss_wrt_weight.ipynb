{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddc3c7c",
   "metadata": {},
   "source": [
    "# PyTorch and Mitsuba interoperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc1227",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to mix differentiable computations between Mitsuba and [PyTorch][1]. The ability to combine these frameworks allows us to squeeze an entire rendering pipeline between neural layers whilst still preserving the differentiability (end-to-end) of their combination.\n",
    "\n",
    "Note that the necessary communication and synchronization between Dr.Jit and PyTorch along with the complexity of traversing two separate computation graph data structures produces an overhead when compared to an implementation which only uses Dr.Jit. We generally recommend sticking with Dr.Jit unless the problem requires neural network building blocks like fully connected layers or convolutions, where PyTorch provides a clear advantage.\n",
    "\n",
    "In this example, we are going to train a single fully connected layer to pre-distort a texture image to counter the distortion introduced by a refractive object placed in front of the camera when looking at the textured plane. The objective of this optimization will be to minimize the difference between the rendered image and the input texture image.\n",
    "\n",
    "We assume the reader is familiar with the PyTorch framework or has followed at least the basic [PyTorch tutorials][2].\n",
    "\n",
    "![](pytorch_tuto_figure.jpg)\n",
    "\n",
    "\n",
    "<div class=\"admonition important alert alert-block alert-success\">\n",
    "\n",
    "üöÄ **You will learn how to:**\n",
    "\n",
    "<ul>\n",
    "  <li>Use the <code>dr.wrap_ad()</code> function decorator to insert Mitsuba computations in a PyTorch pipeline</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "[1]: https://pytorch.org\n",
    "[2]: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec07a13",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, let's start by importing `mitsuba` and `drjit` and setting an AD-aware variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6f3cc2-6edb-425c-bc56-fe65c270d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "mi.set_variant('cuda_ad_rgb', 'llvm_ad_rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d7afa",
   "metadata": {},
   "source": [
    "We will then import `torch` as well as `matplotlib` to later display the resulting textures and rendered images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40f1dcd-9fbe-4dd3-93e3-adce381b696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805bde2-8fc3-4cf6-8695-c9552eb5a313",
   "metadata": {},
   "source": [
    "<div class=\"admonition important alert alert-block alert-info\">\n",
    "\n",
    "‚ö†Ô∏è **Note on caching memory allocator**\n",
    "\n",
    "Similarly to Dr.Jit, PyTorch uses a [caching memory allocator][1] to speed up memory allocations. It is possible for the two frameworks to over allocate memory on the GPU, resulting in allocation failure on the Mitsuba side. When running into such problem, we recommend trying releasing all unoccupied cached memory of PyTorch using `torch.cuda.empty_cache()` which should mitigate this issue.\n",
    "\n",
    "</div>\n",
    "\n",
    "[1]: https://pytorch.org/docs/stable/notes/cuda.html#memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e85c",
   "metadata": {},
   "source": [
    "## Load texture dataset\n",
    "\n",
    "\n",
    "In order for the fully connected layer to learn the distortion mapping rather than the distorted textured itself, we are going to train it on multiple input texture images.\n",
    "\n",
    "The following code loads a few squared images using `mi.Bitmap` and converts them into 32 bits floating point RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fbe367-016b-411b-ad41-fa24bc77518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "R = np.array([1.0,0.0,0.0])\n",
    "G = np.array([0.0,1.0,0.0])\n",
    "W = np.array([1.0,1.0,1.0])\n",
    "has_transparent = False\n",
    "\n",
    "R_opacity = 0.4 if has_transparent else 1.0\n",
    "G_opacity = 0.25 if has_transparent else 1.0\n",
    "\n",
    "r_front_w_back = R * R_opacity + W * (1.0 - R_opacity)\n",
    "g_front_r_back = G * G_opacity + r_front_w_back * (1.0 - G_opacity)\n",
    "\n",
    "# final_color = g_front_r_back\n",
    "\n",
    "final_color = R\n",
    "print(final_color)\n",
    "\n",
    "a = np.ones((64,64,3))\n",
    "a = a * final_color\n",
    "red = mi.TensorXf(a)\n",
    "def display(image):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    ax.axis('off')\n",
    "    ax.imshow(dr.clip(mi.TensorXf(image), 0.0, 1.0))\n",
    "\n",
    "display(red)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd857493",
   "metadata": {},
   "source": [
    "For the sake of simplicity in this tutorial, we will assume that all texture images have the same resolution. Moreover, we will make sure that the pipeline renders images at that resolution to simplify the computation of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896819e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  s / (a + s)\n",
    "ink_albedo = np.array([\n",
    "    [0.05, 0.7, 0.98],  # Cyan\n",
    "    [0.98, 0.1, 0.9],  # Magenta\n",
    "    [0.997, 0.995, 0.15],  # Yellow\n",
    "    [0.35, 0.35, 0.35],  # KEY: Black\n",
    "    [0.9991, 0.9997, 0.999],   # White\n",
    "    [1.0, 1.0, 1.0] #Transparent\n",
    "    ])\n",
    "# ink_albedo = torch.tensor(ink_albedo, dtype=torch.float32)\n",
    "\n",
    "# a + s\n",
    "ink_sigma_t = np.array([\n",
    "        [9.0, 4.5, 7.5],  # Cyan\n",
    "        [2.5, 3.0, 10.0],  # Magenta\n",
    "        [2.25, 3.75, 19.0],  # Yellow\n",
    "        [5.0, 5.5, 6.5],  # KEY: Black\n",
    "        [6.0, 9.0, 24.0],   # White\n",
    "        [1e-4, 1e-4, 1e-4]] #Transparent\n",
    "        ) /20\n",
    "# ink_sigma_t = torch.tensor(ink_sigma_t, dtype=torch.float32)\n",
    "\n",
    "weight = np.random.rand(6)\n",
    "if not has_transparent:\n",
    "    weight[5] = 0.0\n",
    "weight = weight / weight.sum()\n",
    "weight = torch.tensor(weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72137d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ink_scattering = ink_albedo * ink_sigma_t\n",
    "ink_absorption = ink_sigma_t - ink_scattering\n",
    "\n",
    "ink_absorption = torch.tensor(ink_absorption, dtype=torch.float32)\n",
    "ink_scattering = torch.tensor(ink_scattering, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde2511",
   "metadata": {},
   "source": [
    "## Scene construction\n",
    "\n",
    "The scene/setup for this experiment is straighforward. First we instanciate a `perspective` camera that points to the origin where we place a textured plane. Then we place a `sphere` object with a `dielectric` BSDF which will distort the textured image when viewed from the camera. Finally the whole scene is illuminated with a `constant` emitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5caa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitsuba import ScalarTransform4f as T\n",
    "\n",
    "sensor_count = 5\n",
    "sensors = []\n",
    "\n",
    "for i in range(sensor_count):\n",
    "    angle = 180.0 / sensor_count * i - 90.0\n",
    "    sensor_rotation = T.rotate([0, 1, 0], angle)\n",
    "    sensor_to_world = T.look_at(target=[0, 0, 0], origin=[0, 0, 4], up=[0, 1, 0])\n",
    "    sensors.append(mi.load_dict({\n",
    "        'type': 'perspective',\n",
    "        'fov': 45,\n",
    "        'to_world': sensor_rotation @ sensor_to_world,\n",
    "        'film': {\n",
    "            'type': 'hdrfilm',\n",
    "            'width': 64, 'height': 64,\n",
    "            'filter': {'type': 'tent'}\n",
    "        }\n",
    "    }))\n",
    "\n",
    "sensor = sensors[0]\n",
    "spp = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707da8eb-5f05-4053-9452-f64ce505446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitsuba.scalar_rgb import Transform4f as T\n",
    "\n",
    "scene = mi.load_dict({\n",
    "    'type': 'scene',\n",
    "    'integrator': {'type': 'prbvolpath'},\n",
    "    'red_cude': {\n",
    "        'type': 'cube', #red cude\n",
    "        'to_world': T.rotate([1, 0, 0], -90).scale(2).translate(-0.5),\n",
    "        'bsdf': {'type': 'null'},\n",
    "        'interior': {\n",
    "            'type': 'homogeneous',\n",
    "            'albedo': {\n",
    "                'type': 'rgb',\n",
    "                'value': [1.0, 0.0, 0.0]\n",
    "            },\n",
    "            'sigma_t': {\n",
    "                'type': 'rgb',\n",
    "                'value': [1.0, 0.0, 0.0]\n",
    "            },\n",
    "            'scale': 20\n",
    "        }\n",
    "    },\n",
    "    'emitter': {'type': 'constant'}\n",
    "})\n",
    "\n",
    "params = mi.traverse(scene)\n",
    "# key = 'textured_plane.bsdf.brdf_0.reflectance.data'\n",
    "key_albedo = 'red_cude.interior_medium.albedo.value.value'\n",
    "key_sigma_t = 'red_cude.interior_medium.sigma_t.value.value'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810bd5f5",
   "metadata": {},
   "source": [
    "## Wrap the rendering code\n",
    "\n",
    "This next block of code is the core of this tutorial. \n",
    "\n",
    "\n",
    "We define a simple function that takes a texture image as input, updates the scene and renders it. In order to use this function in our PyTorch pipeline, we need to make sure that PyTorch knows how to propagate gradients through this function during the backpropagation phase. For this, Dr.Jit provides [<code>dr.wrap_ad()</code>][1], a function decorator that automatically inserts a custom operation in the PyTorch `autograd` system when evaluated. Under the hood, this custom operation will call `dr.backward()` internally to propagate the gradients through the rendering algorithm and properly assign the resulting gradient to the input `torch.Tensor` object (here the texture image resulting from the neural network evaluation).\n",
    "\n",
    "In this tutorial, we are inserting Mitsuba/Dr.Jit computations within a PyTorch pipeline, hence we need to specify `source='torch'` and `target='drjit'` for the `dr.wrap_ad()` decorator to produce a PyTorch custom op. Note that it is also possible to use this decorator to wrap PyTorch computation in a Mitsuba/Dr.Jit pipeline, in which case the `source` and `target` arguments will need to be swapped. The decorator will then automatically insert a Dr.Jit custom op in the Dr.Jit AD graph.\n",
    "\n",
    "[1]: https://drjit.readthedocs.io/en/latest/reference.html#drjit.wrap_ad\n",
    "[2]: https://pytorch.org/docs/stable/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec8176f-90fd-45fb-b1d7-b0b445b16cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dr.wrap_ad(source='torch', target='drjit')\n",
    "def render_texture(albedo, sigma_t, spp = 256):\n",
    "    params[key_albedo] = dr.unravel(mi.Color3f, albedo)\n",
    "    params[key_sigma_t] = dr.unravel(mi.Color3f, sigma_t)\n",
    "    params.update()\n",
    "    return mi.render(scene, params, sensor=sensor, spp=spp)\n",
    "    # return mi.render(scene, params, spp=spp, seed=seed, seed_grad=seed+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5f114",
   "metadata": {},
   "source": [
    "We can now easily render the scene using the different texture images previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac391dd",
   "metadata": {
    "nbsphinx-thumbnail": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp_a = torch.tensor([1.0, 0.0, 0.0], requires_grad=True)\n",
    "# temp_s = torch.tensor([1.0, 0.0, 0.0], requires_grad=True)\n",
    "\n",
    "\n",
    "# weight_GT = torch.tensor([0.01, 0.2, 0.4, 0.19, 0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "# red = render_texture(weight_GT @ ink_albedo, weight_GT @ ink_sigma_t)   \n",
    "# display(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689291f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c9f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/6xptcpl97sg4y_z285cqg0x00000gn/T/ipykernel_10910/1317897249.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight = torch.tensor(weight, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tensor(weight, requires_grad=True)\n",
    "from kornia.color import rgb_to_lab\n",
    "lab_ref = rgb_to_lab(red.torch().reshape(1, 3, 64, 64)).reshape(64,64,3)\n",
    "\n",
    "def loss_delta_76_mse_func(w):\n",
    "    w = F.relu(w)\n",
    "    w = w / torch.sum(w)\n",
    "    sigma_t = w @ (ink_absorption + ink_scattering)\n",
    "    albedo = w @ ink_scattering / sigma_t\n",
    "    current_render = render_texture(albedo, sigma_t)\n",
    "    # MSE between the reference and the current render\n",
    "    lab_image = rgb_to_lab(current_render.reshape(1, 3, 64, 64)).reshape(64,64,3)\n",
    "    delta_e76 = torch.sqrt(torch.sum((lab_image - lab_ref)**2, dim=(0, 1))/(64*64))\n",
    "\n",
    "    return 0.1 * torch.mean(delta_e76) + 0.9 * F.mse_loss(current_render, red.torch())\n",
    "\n",
    "def loss_delta_76_func(w):\n",
    "    w = F.relu(w)\n",
    "    w = w / torch.sum(w)\n",
    "    sigma_t = w @ (ink_absorption + ink_scattering)\n",
    "    albedo = w @ ink_scattering / sigma_t\n",
    "    current_render = render_texture(albedo, sigma_t)\n",
    "    # MSE between the reference and the current render\n",
    "    lab_image = rgb_to_lab(current_render.reshape(1, 3, 64, 64)).reshape(64,64,3)\n",
    "    delta_e76 = torch.sqrt(torch.sum((lab_image - lab_ref)**2, dim=(0, 1))/(64*64))\n",
    "\n",
    "    return torch.mean(delta_e76)\n",
    "\n",
    "\n",
    "def loss_func(w):\n",
    "    w = F.relu(w)\n",
    "    w = w / torch.sum(w)\n",
    "    sigma_t = w @ (ink_absorption + ink_scattering)\n",
    "    albedo = w @ ink_scattering / sigma_t\n",
    "    current_render = render_texture(albedo, sigma_t)\n",
    "    # MSE between the reference and the current render\n",
    "    return F.mse_loss(current_render, red.torch())\n",
    "    # return F.mse_loss(current_render, red)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a466602",
   "metadata": {},
   "source": [
    "## Optimization loop\n",
    "\n",
    "This optimization loop is similar to the one you will find in any other beginner PyTorch tutorial.\n",
    "\n",
    "We first initialize an `torch.optim.Adam` optimizer and use the `L1Loss` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcd5eb0-dd1d-4a58-b2d6-84997ebbfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_delta_e76 = True\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam([weight], lr=lr)\n",
    "\n",
    "# Optimization hyper-parameters\n",
    "iteration_count = 100\n",
    "spp = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ac7641-7401-460c-a855-985e6fe58030",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS: When running under pytest, adjust parameters to reduce computation time\n",
    "import os\n",
    "if 'PYTEST_CURRENT_TEST' in os.environ:\n",
    "    iteration_count = 2\n",
    "    spp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b89cc-0ca4-41db-aed7-37abcf9c1c4d",
   "metadata": {},
   "source": [
    "At every iteration, we render the Mitsuba scene with the different pre-distorted texture images and propagate the gradients through the entire pipeline using `loss.backward()`. Thanks to `dr.wrap_ad()`, the gradients will seamlessly flow through the rendering algorithm all the way to the neural network weights. We can then call `optimizer.step()` to update the neural network weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86240e8f-e7f2-4e2a-8362-f92a164623d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 81.36811065673828\n",
      "Training iteration 1/100, loss: 81.36811065673828\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 100/100, loss: 45.378143310546875\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for i in range(iteration_count):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(weight) if not use_delta_e76 else loss_delta_76_func(weight)\n",
    "    if i == 0:\n",
    "        print(f'Initial loss: {loss}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss)\n",
    "    print(f'Training iteration {i+1}/{iteration_count}, loss: {train_losses[-1]}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ac9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_mis = mi.load_dict({\n",
    "    'type': 'scene',\n",
    "    'integrator': {'type': 'volpathmis'},\n",
    "    'red_cude': {\n",
    "        'type': 'cube', #red cude\n",
    "        'to_world': T.rotate([1, 0, 0], -90).scale(2).translate(-0.5),\n",
    "        'bsdf': {'type': 'null'},\n",
    "        'interior': {\n",
    "            'type': 'homogeneous',\n",
    "            'albedo': {\n",
    "                'type': 'rgb',\n",
    "                'value': [1.0, 0.0, 0.0]\n",
    "            },\n",
    "            'sigma_t': {\n",
    "                'type': 'rgb',\n",
    "                'value': [1.0, 0.0, 0.0]\n",
    "            },\n",
    "            'scale': 20\n",
    "        }\n",
    "    },\n",
    "    'emitter': {'type': 'constant'}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9affe01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1334,  0.6863,  0.6082, -0.1412, -0.0905,  0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46db09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = F.relu(weight)\n",
    "w = w / torch.sum(w)\n",
    "params = mi.traverse(scene_mis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d356c55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAEAAAABAEAIAAAB1mzrKAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAzfHTVMAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAACR0RVh0Z2VuZXJhdGVkX2J5ACJNaXRzdWJhIHZlcnNpb24gMy41LjAiQ6vRVQAAAKVJREFUeF7t0aERACAQwDBg/52fEVBcTaLrumcWofMK+MuAmAExA2IGxAyIGRAzIGZAzICYATEDYgbEDIgZEDMgZkDMgJgBMQNiBsQMiBkQMyBmQMyAmAExA2IGxAyIGRAzIGZAzICYATEDYgbEDIgZEDMgZkDMgJgBMQNiBsQMiBkQMyBmQMyAmAExA2IGxAyIGRAzIGZAzICYATEDYgbEDIgZELs5awJ+FrkS8gAAAABJRU5ErkJggg==\"width=\"250vm\" />"
      ],
      "text/plain": [
       "Bitmap[\n",
       "  pixel_format = rgb,\n",
       "  component_format = uint8,\n",
       "  size = [64, 64],\n",
       "  srgb_gamma = 1,\n",
       "  struct = Struct<3>[\n",
       "    uint8 R; // @0, normalized, gamma, premultiplied alpha\n",
       "    uint8 G; // @1, normalized, gamma, premultiplied alpha\n",
       "    uint8 B; // @2, normalized, gamma, premultiplied alpha\n",
       "  ],\n",
       "  data = [ 12 KiB of image data ]\n",
       "]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi.Bitmap(red).write('red_GT.exr')\n",
    "mi.util.convert_to_bitmap(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d5df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAEAAAABAEAIAAAB1mzrKAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAzfHTVMAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAACR0RVh0Z2VuZXJhdGVkX2J5ACJNaXRzdWJhIHZlcnNpb24gMy41LjAiQ6vRVQAAH8RJREFUeF5VnF2a4zpvhAGQku2eOevLZrLo5DnTbVskgVy8ro+TvpiftkyRQKFQACn58/nf//1f/5UZYebeWu+9r7XWGFVm7mZjVLn33ppZa2NkVrm7R5hlztmae1Xv7u5jVK211lqtteZu1rt7VaZ7xJxVZq25Z44xZ0RVVWbvrfXOaBFmVWu5u3P/1tbKjIhgzLWui3ut5V5lVhXh3tp5mpllMqp7ZmuMY2amkTMj1uJ/me7nmWm2VkRVa61FvF7M0yyiNbM5NYK7WSZjZ651v7u7//xkus8Z0VpE1Vq3W++9V2VWmZmt1buZe2YE6696vcwye+/HEfF6reXempn7WhiY6VadZ2tyRhXLcc9kiiyYIasyM1vjuswI9/e7aq2ItcyOw12/d68aIyLiPBlnrYjWMtdiDEbh262tZRaB2TD+7WYWwZLM6vOzFm7vnRHM3MfINHN3d18rs+o8q8zmlKvcqxh3Tr6V2VpEZlVmmZm7FXNgtF+/sEdVppnZcfSOTZjFnJm9u7Mes+uKcD+OTIHr8ahy72OsFdE7eMuc04zprOV+nrhES6m63VgGSK8KfppZ1fXuHaNxPRhjmn//O5PIaE144goM6p7Ze+ZaZu5EkPt1mZn9/FRF9I7z3cdojRHcwXhrc7qbrcWYIL41nCeQYZbewTR/siZ3Ihe3Moqzknq9IoDaWmu5R1wXKyEiiA9c6W72fme6c28iSeyBS3Dbx0sgA0RnuvceIUMyRETmdWVWRYD144B+1lrruqrAZ9V5iqBAJaZZy8yMcUGNQlrEA0ahCHeuqzoOs8zj4J5mrR2H2RhjVLV2XVXux0EMKU6BAfOugjYhvOuCBFtrrQo341RWYjbncZi5v99zEqvQ4JznaZZ5XVWtzQlhVY0BBSq63CPWYmTWiA1aw9buXG+WudaHCTFS7/UJs43YCPv8rFUlZEJTXI3hMCxx9H6vZXZdQiZTiMjMxCnCltxThTkwBBh3773KbAwzeJgcY/bvv1W9z6mFEHPQReZaYnDN5zwjIEEcxDzMIJOItcZ4v+esAgxVY8yJSdeqgo6AlP7mLoqzzCpyZGuARi5mFWsROe5Vc0JRrUW0FpCIAp0FVRGIEeepKYCj7RLCFCfhb8iDW0IGoJm4MuM37rA001bUgZXMOefkfgR6FYaLOA5ctVbV7SaMc0+zOYFCJhmGJYNEuYW5A5cqRSF3gih6v91YtySE2dcXDgCgEVgIB6/VmkgSGQN7kAeAh0gLlwFZ8pHZnHMGRoWd5xSDo33M3m/csRY4BNNiSVCyKabqPDWtT/oqeG87yux2Y5oyxU5LZkTCGB8XuYcHOUhLYxZkJBxDzsBsRM9a7pAldIT5zNZaq3UzD/Ab0dpxKOb2HHEv+OX/pGasQzK9LugqM3MM9+N4vQQHsYrUTlXvMr87qg57BRyJQFSSYpFjbBOP4U7okZLH0GTNzhNk49U5/w5VsIMqUaBGkC1IjnOCLFEHdz9PhN+4cuUiPjDw+40WigDNok8SYJVwW3WemXLMWpmaSbhZ5e1mZnaeGBd5DBAwEiZnJDPmjPZCO2UeB1nGrIrVmSEBcB7RSgS4m93vfI7ay0TcZ8ZOh8rh7u7uSDD+BAsypHvv9zvuEDFp2ccBdZjN+XxWtcaNdqLaI0UgTFtzR7QRDSBwDAJftMY8kayiLhHYWnIwaJTuAs/u1yUESyYgrCEu5sTvMdn77X4caBgpKDRSZmtrQSNIFcmNMc6Tqgi6xqGAtKrq+VxLNRVQfzyqxvgEI9MVJWgieBUWFnNKs4MRTKCUpTwCWRwHweq+FiLsOBTGOx6qMq+LSYpY5rzfe1f6Og4mzDznnJOELLkpcwMEyiqc3Br1DfAgt/z8kBAhXUkNknxrFHS9r5V5nkApE0cBP7geKkEREYdSO1iktSryViapHAqCb/gUaHd3s+vqvap3BmYBTA7sM0Gwq4jIjED8ZeJ1kiFBvFGLlOP2aHZcyHdfryrpAVhbKREDCZmEvdLaecLfve+CB6yOIZ1CTFwXCstMUIACIBhMJjBQq6+11nHgWndkK+Kxd+rd40CSqw4HEBG3W0Rrt3sEqSccQM0pyYvbzYhS8kePwCNKhZn4B5bFe6gkwkcUIbyq2P/QUM0J27ZGhPRehZaHNaWqvr8RiFxJaUa8UZcIxRFrgXgzKIQiCDSRfzAcUQwY1gIKgOE8Mb7qGyKbFVRdF40UHGH2t6kYn9Vq/cCHv283s+OAFsmMVFSKfSKYmgbr7rVR/HYZEpGVOSdKR7UiPlZYU78SUiReEriYHRpCqsGPQrKioIr683arUodkTvfeuQtpjb6K5B0OrXJHLEhpbKEpxIrPMS/63ey6SPxqs0RcF9+tomxk9tL2uxZq7XbbCh6Y7Fqpikqc9ZMf3bGQyMgd4pV0YHa4tsqsQy0wEl/gT6ZEq23niTmZDoZTOoRYWKBqCVAApsT6Qh64ztQ157lRPwZ3R9K11jtVKCMgmVkScpZwfr+l0uDi3T7TrCPOEzcLLFW3GxqGdWRWnWdE71IoZpnvN4aUCYEF48tiVLjUPozP5/d7a1T57u83IM9UZ4o2RkhCMSX5f2PFnXQsE2I+jP5+7zYFQcYyYGIWdbuJLRW+Y1R5gEjqDHgW7XSeMk3mcWSagV6yAGSphpdUkJQ19IE51HcRVkWSarFR5qm22OYViiVbFU/kOvQN5pYO5DvgWZWQiI5CVLFKxsIF0nEdpgLHJLP3++8CW2qBxLgW7Tmlw7WkSaCE2601VBE3A4GZvbPI3iVUVSGLnmhP01GkjUGQznm7YUawpmoWimF+RPCOPppcij8pl9drLcnfXaPO+Xphfszq3nvvc65F9aOKt7Xj2EWZKJF7ADAgwdjiC9zEbzdAzAQC9/5+Z2IOuI2OdhXmIqWAJDguc7eTzhPaIVi3YfgUE6nmlBoBmaoDMYNax9ASbAzPYhjNjdAmHiURcDWJkHElUs3QSxobE4taIo6DbETuAaWMoKtYCd9jZvCB7i7oSruxSuIK2PEvukvMB2XFuK21Fpu/8Rt+xYzCJ7FwHFAPuBEtSLpyE77L9NVXkQmkEHZtvNZxQAVVqKQIVD+RxLwgOgGCBeOa3nvfpRlmUlrGMTRUSM/kADZ//u7/ikKJTMAGfpm51hANQ5MnBA2iBCvhHNYgyiWjmCFXM2nDmUU8HlVzdjS48jymgwsxTKYKblGKUiqiUNioYhuEipElgbTe+S4Lae265qy63ynIMU5r6GiKflWM3JesAGsSvuJVZQiJYfQ+RSGJHchsslLhBv6eTzINWYdIhMIy1/r1a7fV1qqqBDhjZK51v0tgKPHKIaJY5gnrq7m5C9aq61qrKsT1LEAf8jv1Vkg5uh3LcJdiud3dPTANAfe3pqLDIyRBPKBMbA67R7xepE5wF7F5VvCghJFM5oex1K5jyUQcxFOF2yjAFANc/XhAaGuZsUHIyhlX7iMPaEzRtRgf4EHjUnyZY4yhHhPyHuL8+iJTEmGZZp+kx5S09Yh/pLhZhFpyCvcqtkQotCqPI8L8uqrAlMaBMQlryO7XryqEI0YFSeJYM23Q7DTnTpol3MFyJjM/zyqMTiWBTBgDt/Idvo+OoqHGvDAsLQ3WKP2kXS6uVU1DgQcZsi9tNucYHzYPKDvTrPfWaMYBMfEEbgS4n0aedpowKUagPWAGH6uGZV9J7hA3KrWSvI6D/dsqaWH2gSXoMjPZ8oaDzagYpaOZFkEN6pm82uFE2HVRnxApuAY34HLSIeapUnEnNaT4rHq/FS8qGM9TpBWBXKDgRJqQd8Zg1eg0pIDig/GxCe4jb82JSxmTPbWPMhQeQBVVHB4X9lVkw+MymXat4F2wCe9Kj7CFAlIxw9cXmJf6ATf0ktgIJK9gREQh9Efa650S7P6IZi5T7nne76gaMpZ/fgQayjFWFlF1HK2hhjLZ5GElxG8VW5rKJUSwWSZXYkKoeE7ouzXojvmv1Xsm8cUGpzu88nho9e4BruA5KkMlYMiBpYEsggrhSTCuNafISi4UrxJZ8C+h/f2NcqEnQ1xIFRBB3FnMTVOXAGdhiMtKKyu6lccBr/fOXoIIbyswurnnebtRqYvYMCGYhSirqtYa434XxgGEOkKMqU6xHCaZSsJVkSWCAybKr7QRafPQ0IkdlK2pi+0uNPNVFeBVVTR4Qd7jIRaHENDqpMrWMt3PE+2ESbixO/lETuS+mu7rFXGeEA6LZLKqV0lyGMhdxRGpnoUTmT8/ZASVSe+3lJXihfvP+c8/VXSEaBSKilnZWu69v16ZERxaIUIgXsDIZgupGN03JylfwkTyRh1U/a4qkIMREWXSEJtwFAcMbIZUo3toRiIiU+jA1lprkZBI4IjcTOUEOYyKgkbFeUpmgkz3379RSMIqv6263xkrE2Kser/JOigPqoxM99+/3emwQiPEUMTrRXeLGbq39u+/vaOCYHPVI/e7qn/qHjgAUlUDgpNN2AxpG5+Slj1mLKi2R2scrhFQqo4j/vyp6v265rRCJPKD+Ul0KHAC8Dh2R0RKQP7kBpyPYLcWc6vGgMyeT2UFhTDKhDF1noxxCXcAwYKBh9Kq/sQ0uFuksD9F/yAutHyiFfDtqhyJ6r7W+23GJpK7O6QkOErftcaniA7imNqbJMy83m/upLqEvzm/lBlMS1uNsC0IM1NiYjEgDhmGt5nGGHOq1Pr6as3diuBjYqR5JhyhJAfz0y05Dh21YpGZ39+ZmWq+8U3yhRwi3XK/V10XJrnfOe6UOQZtPsDB8qGXqir3aNE8AApqBkSzO43QeD4hQLSYGfvfKCWgptzHVlbmWlkkXVUTc7Ih1bvVVoJVQGrOtQLtz7QpZ6AKOFFbya0xFXbBZBjyBHKPdEbyXklXh5HBiDqYO0KYCKYjQbVmsI6fJ/JPu28yHItGX4HH9xv2BVNgsne2YDAFmp97RIDl3sLD2RUAEIpXCE3RKUrELZjy9SLvZHL2QqXjebqvSeWLO5njcURkRlMqlkwn13ZCmr4Ny0Rc4nPp7bWqHg+8Z0a6ldIgi/z5A8+ZrXW/V+E2kRBRhEh8vdYiVHejgLCvsoKhMR2xhVHM9DdplX4R7C8FgpEpGDEfxtURsQha6GZQok4Fcg3zj1iLk3GCSAR2aO37G6RHcGIKsEZcFwkdi/FNd/bKFLnYCzjoCIxZxKeBwCYBqJGkxAQEO25BRUASSo1EDRgh3CAxiIGktZ0AOiNQxNqthbAw5L6T5B4UhplETMhBNQbQWdpHluEyq8bovTXyFhRAGYgsJhZZO2MiBhhvDMhlTvaBOX4psX4cvbN2gAW9agQqeXq6rJAG3VrsfmwofQoxZJK2Hus/dSCTRkrSo3+/8R5Fivxq9vVFCqWCJi1DU8jY+DAuEo0+KJPiE53C1sYGTTjVmmYchCEegQFmJjmjVWDta0APZpATPzLFru2JeEbU2BAYrsThFKUYjZ+vL0ytagZKiaBcfTyIK9aAHXrnTDXakrkrJjuq2T1zjN6rWhfqNIUxfv1qrXfaqRRQIFXBj9Sj6WRm9j//U4VURbdzLV3EvYPGeQFwIK6fk2YAaoFSDzLRckj/amNgZu5jn3PK7T+lE/NjfE77KBNRNFKJ86nkdQTHUOQC6iR3xISkByNrfSq2qrTV2j/U7h7BXhin5z6r8IjW3avWDOhhjIjHY4yIFuRq0ilMicfZFbouMrlYm/ggPNWaOw6z4xQOUM04aTvk+xvG3o5WCGsUmZ6Sjm8i5jAJ2OW4PDp90yPpEYcpFx1HJifaIApdyZlnDEnvHm2m1W0YATAaD7oSF4F0qqAI3IFTNTJWczcbo8Va1392OT6srScDpMfB1XEoGqSet0FV98rLZq29XjJMfcaDHnR7dLfUyJw6PLJrDhFQBNhjbpzPAH+4jbP5GJcu666WpdgEEWUfKb216FYqP2WSrolvKTPOQ9D/x8AcAHi9NoEAJs5VAIH3m0Qum9G0AEZahay2VtVHOhIcO23AW9ISOhfN/zYFwKeEJp7+iLzOhMG2O7tC4ko4GezNuRYbi2CWNobwxhIoX6RRMpltfuKEI7G0NJSsmTedInB5nr2/XmZIAG2h4EhcDlygqSxiCCPidCVqzE4RqpzEqSS45Dioq5SrBDkMLhKEdcwCZpQoYuqkYYJaZxlZECLTTD1FVC3YYenIRko2xlyLoyVyJswr4oJNqTOIBml+VaH8W3qJWV6XmX3cgeMhFmJC0Ypp6NdnjvH1JapaSwKUvSvSo0Cyln8ikYQME/RO1cPI2+D6kSZU5cRaYX+ppfP89UvRrkKyo1hgRA0qlt0sTyqDqf//doWQfp40ld0jxqBHop6hiEvlfwTbMs8nzQMCX3108Ij6JtlFgFNpD2YJ4tHsXLc7okQYplH3lrtgQOGT1VapGdHapglEAZDEbcAQjaSeGb2utTI9+tH693cu/5ycI1HjQvLOnz+KILPbLWKtT6pBs4NTFWFVSp6cz2Gp4Or7G8MqJliyjhFS+6mIqmK7UW7mqjEijoOu/c8Pyhn6MtPZCaG/ikO3yhIYgiOz2joHc0QcOkfRJJeplY5WqdIRKfCdScajabiWaASAjEGfCGKm42TGocqqT3GVuSrhD0CkgktuFxRZK3Ta6WVi7DkxG19SihUud3KDdDJhN7jbHUTRatrVLO7htPNaUjg6V4EDuRJTZrK5r8IMs4uRSczaeoTczHQelCu3cMWAqBTow/28RSubozIKQ8pNzDYCw/Wuo4tqZJh9f68VcbtxWolMSK6SZHBnkwa25+ERxoeItnCloooIOu9bElKMoCY4W/DZBPmEG78nQUNFtxtmx40sU7nejLTOv9WLVyQpPfWeeV3gwt2ME5djsHRYHbGIOAbt4Pf14i6tMRPyBdFMw2vOMUDeWpm5cq3P+bveNVvAw3bp/U7Fe12tkVfodNLfBcXaMzfrnXaHOmG05YljHgABrOdJEgZoVETUIQH61hqDDC4GXIvz78Im9TCJDabFcRKhLCaCc9RlmkTvnJRWZ4nA5GqkH3QkzYCj3I9zu0R4Zjm4sD4IrprzuubMfL3ICoxP22+3CIRVOp+KcyEfzheOaRVAnxKm6hlXabacoRM1nqf7cVAnU07SiAYw0FAmGzXu8M6cwbTgfZ4JoSEBNUkVubP5RyVLQCttYzIFujt04Z8TO+5rjcGJSfnfPfPxcDd7PNDv3GtTjpl7/GcLx52uOvpJGgzVgalhYa7lDhLKtArMUPvcZS0oAhRKuFZBkKyfSD0OZTL31nSm2qzq8VCeoDE9xnW5q0NMlKAAiR4eK6ca0T6xmVmAZXoamZzOhLOE0KrbDbHpLpUBYpiK0qQ7jwpJFOISPP58Vunp9dcLAjHj2RjoDaJQ6jRjxwsDYzApoo+DPYICy4ynzqEkIhangllqYH0fGJBRZGaioLXzZM7M6LoEGvf7nVy4FseN+ZS6gdF1fOz1IkqIK1xTZfa//6tiTE8jfGKUm7AbitjDDSB9rdaOgwTyes35aTl8Tq6xeKVwXKNCA7rBJO63G4FMgqSfqTha63aDg1UUacOPUNWzmJkRuIn2A8sh7h4PRSPXScFosxCWZ14iSpGbToAwGsAhvSvFsounSptHr4R0uP9v1QXyISXAYTbn79/uVb9/b3keUTXnh07UIIggBqg+xZLEwn+aDBX+9WVG4lEKq1LZTjoTdYAd1Qy8wqJ3XlXBHVSUwfh0nlBBlEvqPrJs7qgzdK1xkJ2tSjMzPaAKVyOyMQcOWMu999+/M93/+UdS2ux+r+Icqr7N7Nj8OY4qilaz5zPiPHEzZ8N3raudC07+6ARUBC9ZqPr+3mAC9F3eENdjbCamFEbWfjz253uBlFJob4hC6h8sU53q9LJ+QJ9CEUH2+7fqQ65kUUwT08/JeTqeueTJBq6DbTHZ1xdQyRyDWCFOdRAm0+zffzMzHw8ERCYuxOVkNCintZ8f6S4dKBYVmtEKJFOxO6BIZgX//EMOqDJ7PnfqhxAfj9YiOq+M2O1fJT6ZAs8KPzBkJsxMuqOO5DtrtUYM0G8hnpCBfLc1DrO8Xmh5TZY8oa1BdRnNkAY6q0C08i8OJeLutargcEgUYIiUkKXkC6KRB+2INp3akFJRjY8bKb3UXibqzBSBEg9E2Fqc0qBe1xHj+91MmQK5TBy+Xmut9ZeiEQsqxUEFQr1aEdpck7oQqrj+8Vgrkx1dTrqZSdhtZM8Jj/OYEYqEZej1NsyK/qX951OcqIYWbY/XCxOgsDIR0vY5ozCnWWvP5xaxuIUfTHCevbN9CHiE/0yzOVWjUFOIMCPe79Zaez5RR2Z61mzrPWY85xhq1wFeoo66xz2YFklxN80wlB4e4gbgka/RH9VbUaRq6eH0/nrxfVHRbiIzmrteKsOWPYhFZvbe2u2GmUi/vWXuw09SXxHszbG3xYsDuEvvrfVGtuC8wv0ud7I69Mla39/uKHlc6t56VesYq4pSzYxD84iILVLpIiudRnx/bwXHOzbu96qy84TW1KXCjWqHdAWEEiSLU5+RFLV1A2H4aT65NhrNmMrfSoJP3u/zFEvD+zS4iAbG5kkVqQI9dwYDQ0atZT6f0IQZLQ0MClh4jEkpF0MptylGt0zlO2a9f30R/RCbmXtlVSXxhNwUjrUqFXVb9VDhzsnuM/AiKiPM9ODu1xfNSupt99Z4CKUzBMsQpVRxaJC2hLbxwD8LoMEVQZkNgfV+HChl3kfF4saA8TUap0OVXM3o2is10WcSRVD4cCeBgNQqaSD2ZrFrrTUnpZI7r9TYZx+0Y5EpE+Au6Za13CkMRYEIWXfBj1nyPfIcUX2/s+dBGjdDSYktKGCviz4SABIo1gqEo9qqUj3aRnffNSV/8lVUjW5QpUfYrgvWaw35xdSpA4QhNmHk6IiI55NkrQaAGYkTQ6qHL8QR3hJ/ZmuRphWT6DIAcxzQnNg88/k0c388WL7EAi9WqGKkKioGYZ8for8Ku5B4ScrsIdI3JcaIdrXTq/ToFY1xd54wNjML2gCg8TyrVJiQ6zEBIaf3ne0D6KIWs0wdUzRjHFEARCE8VKl2pYdCr4lkS3iCSuIHzS7dDOJpZrPPZUbdQA9LkakIMtN5HK47DvSSOykUd2tkTIxxYXzM7E5dTQ0M9bGKKmUKnmQGFpgfdYXK2mxAF0D9Lhz/QXTvNMnAGi0HUEo0wHxMEse4n6eOQPFbUrA77WKkp0zHddAA3RU2L1m03LyjjZzAVgexCNXsfg9JPIJn6jnSwqfcj7u/32bnSZuAiFSCp9CUtMRMZr9+RdBW04ujzhMKY9uIp+BkYmUuIhiqhqC4O51m+rgiWzmOatu9Y1Sogy8yMK20iExzjCNMVKEO2KzWu3REWVvOgnmMjhswkJRwFZyYOUbr0czmqE+rTk+2YEwznqsSBUBWxAdG3DsWkBXf4UlEohtVzkEt5AauxB2qmH9+NPfeiR7GBGT73JQAebtBXVQXOFYxnvn15U7lzKtIONAvac/L0eJvXSyGN+MtDmDUCgLBKJCUZKu+tZeM2dgczNy3w3EQh1pimc8nxLFmJfqaihp64f48wSKlZJ8f1RngFNdjOu63Z4UR1X/lfXbMhuKSMcX9IjIlf71lwixLTWh0I3nOjEPM0l+SJ6IaDrRBbioTeZkUxWLs50yYPMtWU7d/Dm6hBpRUN01kqho4TzBAD0fCTQvc25liYTb+QBbO+/3b3R2M9K70Ld0FuZB99Pg1jW4WT8UrFpeAGINMEK11M9X8a/F0POkdhKqqx2GMSZzRjsy0mpO2cwQbWVXvN+efgSYaCRCajXFdrZl9fUXoTaXaoMdtmZnxd0tZL6mAlzGmKk5aaGrPqqBorfefHzP3MdhIBJVkejNqQIlFxRfh7N77/e6uRE0jjAobSuNOnMbR/m0EioVI00vPXi8zXmHJKQe9Q4stGbM5cpE1Ho/W3H//pqFsRoKFPiBQpVhq2q+vzOuibkej6U+o1d395wdn4XZWLYmiN60SZzqI76695/7nT9XtRjg/HihWOBdsKgDlN1idd+wgFokE6QGQHsHGPclcjbjN4LywS2NDgHoRElmBRE9JtR+g5Yr3OxPMglNpNok/iAt3E+xSOlTAKpWeT6kstRFxOzt0jwexntn7dbE/IVtgXppxH6L+HMahDag0bebOy8rIbYCKLukny7FdBm+qq7/TDmINnauHkPbt3cegTaESnElclx66EInITbTrcAa4dtfTKCRzdBDZRdPerqWsy2S/gZnIpFJbmJvv7STK/xWHzyfQYIZKq9iA1gmFqN7+xptHOX6LZKY7dBza4xP/R5ipzccKEBqQKnsMv35xbWtmXef0I9z//XfOMUgpNMBYAuHDgulqjMFBXVhytybmJDLM1CNXF14ttu1gECrZyP+USqWaQArEU8Wbe3mFCPjXOByt5FFWopZ3YpOllEnMqD9YB+XhWr9/Q18iRlY0J/0uSlPqFT2ySgUTwcsLxQ8kZx1gZt7QMZYQ6ml3tna/j7FWuCuhzHm/g0rSBQsUi9OYxUQ8wjBn6+4rSeAYkIYwpQqaF0xHyPRyK/qFJhoE8c8/EA5LFl2Bw73logXRu8KQMDmNM5xi9ucPFTdrkCqSMFAx5W7284MozCTfZOpcCMfbqVKIJOl/RTSv0RTWRW3q7EZEs08XgLmpWq/iiYHeg8mBE3j/8dAhbVISZ3pg1d7pboPl8Kpw8XAVW3G80QrKgp9b22+2VbiK+yWD9ebFTRmKDF3n3vvjYYb7t/4nGkik7u73O6kY/gUCiAZ3M3a1iJ/XS2AAcOyL8bqliPM8DmYPWOJjK9oV+8VSkrUIZepiwNqaldXzKY7B8BSX7n/+uFf9H+iQZVyT0XwfAAAAAElFTkSuQmCC\"width=\"250vm\" />"
      ],
      "text/plain": [
       "Bitmap[\n",
       "  pixel_format = rgb,\n",
       "  component_format = uint8,\n",
       "  size = [64, 64],\n",
       "  srgb_gamma = 1,\n",
       "  struct = Struct<3>[\n",
       "    uint8 R; // @0, normalized, gamma, premultiplied alpha\n",
       "    uint8 G; // @1, normalized, gamma, premultiplied alpha\n",
       "    uint8 B; // @2, normalized, gamma, premultiplied alpha\n",
       "  ],\n",
       "  data = [ 12 KiB of image data ]\n",
       "]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_t = (w @ (ink_absorption+ ink_scattering))\n",
    "albedo = (w @ ink_scattering / sigma_t)\n",
    "sigma_t = sigma_t.detach().numpy()\n",
    "albedo = albedo.detach().numpy()\n",
    "params[key_albedo] = albedo\n",
    "params[key_sigma_t] = sigma_t\n",
    "params.update()\n",
    "img = mi.render(scene_mis, params, sensor=sensor, spp=1024)\n",
    "mi.util.convert_to_bitmap(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1337963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9875429  0.57038087 0.42945313]\n",
      "[0.11912702 0.16761896 0.71142733]\n"
     ]
    }
   ],
   "source": [
    "print(albedo)\n",
    "print(sigma_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e17e1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5302, 0.4698, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d40a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5302, 0.4698, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "name = 'mse_red'\n",
    "mi.Bitmap(img).write(name+'.exr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e190310",
   "metadata": {},
   "source": [
    "### Result ###\n",
    "Red:\n",
    "- weight = [0.0000, 0.5854, 0.4146, 0.0000, 0.0000, 0.0000]\n",
    "- MSE error = 0.283 --> 0.149\n",
    "\n",
    "- weight = [0.0000, 0.5302, 0.4698, 0.0000, 0.0000, 0.0000]\n",
    "- delta76 = 81.368 --> 45.378\n",
    "\n",
    "- weight = [0.0000, 0.5107, 0.4893, 0.0000, 0.0000, 0.0000]\n",
    "- 0.1 * delta76 + 0.9 * mse = 8.89536190032959 --> 4.667838096618652\n",
    "\n",
    "Green:\n",
    "- weight = [0.1565, 0.0000, 0.7165, 0.0000, 0.1269, 0.0000]\n",
    "- MSE error = 0.276 ‚Üí \t0.165\n",
    "\n",
    "- weight = [0.2280, 0.0000, 0.7720, 0.0000, 0.0000, 0.0000]\n",
    "- 0.1 * delta76 + 0.9 * mse = 7.025 ‚Üí 5.235"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fac44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a25aa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_red_weight = torch.tensor([0.0000, 0.6224, 0.3776, 0.0000, 0.0000, 0.0000])\n",
    "# predict_red_absorption =  predicted_red_weight @ ink_absorption\n",
    "# predict_red_scattering = predicted_red_weight @ ink_scattering\n",
    "# predicted_green_weight = torch.tensor([0.1611, 0.0000, 0.6950, 0.0000, 0.1439, 0.0000])\n",
    "# predict_green_absorption =  predicted_green_weight @ ink_absorption\n",
    "# predict_green_scattering = predicted_green_weight @ ink_scattering\n",
    "\n",
    "\n",
    "# naive_red_absorption = R_opacity * predict_red_absorption + (1.0 - R_opacity) * ink_absorption[4]\n",
    "# naive_red_scattering = R_opacity * predict_red_scattering + (1.0 - R_opacity) * ink_scattering[4]\n",
    "\n",
    "# naive_green_absorption = G_opacity * predict_green_absorption + (1.0 - G_opacity) * naive_red_absorption\n",
    "# naive_green_scattering = G_opacity * predict_green_scattering + (1.0 - G_opacity) * naive_red_scattering\n",
    "\n",
    "# naive_mix_albedo = naive_green_absorption / (naive_green_absorption + naive_green_scattering)\n",
    "# naive_mix_sigma_t = naive_green_absorption + naive_green_scattering\n",
    "\n",
    "# naive_mix = render_texture(naive_mix_albedo, naive_mix_sigma_t, spp=1024)\n",
    "# print(F.mse_loss(naive_mix, red.torch()))\n",
    "# mi.util.convert_to_bitmap(naive_mix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aff8e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98731518 0.55821476 0.43906476] [0.11929625 0.16711125 0.705335  ]\n"
     ]
    }
   ],
   "source": [
    "weight = np.array([0.0000, 0.5437, 0.4563, 0.0000, 0.0000, 0.0000])\n",
    "a = np.dot(weight, ink_absorption)\n",
    "s = np.dot(weight, ink_scattering)\n",
    "albedo = s / (a + s)\n",
    "sigma_t = a + s\n",
    "print(albedo, sigma_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7ff16a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2750e-01, 6.7500e-02, 7.5000e-03],\n",
       "        [2.5000e-03, 1.3500e-01, 5.0000e-02],\n",
       "        [3.3750e-04, 9.3750e-04, 8.0750e-01],\n",
       "        [1.6250e-01, 1.7875e-01, 2.1125e-01],\n",
       "        [2.7000e-04, 1.3500e-04, 1.2000e-03],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ink_absorption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b268eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2500e-02, 1.5750e-01, 3.6750e-01],\n",
       "        [1.2250e-01, 1.5000e-02, 4.5000e-01],\n",
       "        [1.1216e-01, 1.8656e-01, 1.4250e-01],\n",
       "        [8.7500e-02, 9.6250e-02, 1.1375e-01],\n",
       "        [2.9973e-01, 4.4987e-01, 1.1988e+00],\n",
       "        [5.0000e-06, 5.0000e-06, 5.0000e-06]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ink_scattering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
